diff --git a/.idea/other.xml b/.idea/other.xml
deleted file mode 100644
index a708ec7..0000000
--- a/.idea/other.xml
+++ /dev/null
@@ -1,6 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="PySciProjectComponent">
-    <option name="PY_SCI_VIEW_SUGGESTED" value="true" />
-  </component>
-</project>
\ No newline at end of file
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
deleted file mode 100644
index 94a25f7..0000000
--- a/.idea/vcs.xml
+++ /dev/null
@@ -1,6 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="VcsDirectoryMappings">
-    <mapping directory="$PROJECT_DIR$" vcs="Git" />
-  </component>
-</project>
\ No newline at end of file
diff --git a/cnn_models.py b/cnn_models.py
index d6ea889..843be6a 100644
--- a/cnn_models.py
+++ b/cnn_models.py
@@ -91,3 +91,90 @@ def resnet8(img_width, img_height, img_channels, output_dim):
     print(model.summary())
 
     return model
+
+def RNN(img_width, img_height, img_channels, output_dim):
+    """
+    Define the model with RNN architecture.
+    
+    # Arguments
+       img_width: Target image widht.
+       img_height: Target image height.
+       img_channels: Target image channels.
+       output_dim: Dimension of model output.
+       
+    # Returns
+       model: A Model instance.
+    """
+
+    # Input
+    img_input = Input(shape=(img_height, img_width, img_channels))
+
+    x1 = Conv2D(32, (5, 5), strides=[2,2], padding='same')(img_input)
+    x1 = MaxPooling2D(pool_size=(3, 3), strides=[2,2])(x1)
+
+    # First residual block
+    x2 = keras.layers.normalization.BatchNormalization()(x1)
+    x2 = Activation('relu')(x2)
+    x2 = Conv2D(32, (3, 3), strides=[2,2], padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x2)
+
+    x2 = keras.layers.normalization.BatchNormalization()(x2)
+    x2 = Activation('relu')(x2)
+    x2 = Conv2D(32, (3, 3), padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x2)
+
+    x1 = Conv2D(32, (1, 1), strides=[2,2], padding='same')(x1)
+    x3 = add([x1, x2])
+
+    # Second residual block
+    x4 = keras.layers.normalization.BatchNormalization()(x3)
+    x4 = Activation('relu')(x4)
+    x4 = Conv2D(64, (3, 3), strides=[2,2], padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x4)
+
+    x4 = keras.layers.normalization.BatchNormalization()(x4)
+    x4 = Activation('relu')(x4)
+    x4 = Conv2D(64, (3, 3), padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x4)
+
+    x3 = Conv2D(64, (1, 1), strides=[2,2], padding='same')(x3)
+    x5 = add([x3, x4])
+
+    # Third residual block
+    x6 = keras.layers.normalization.BatchNormalization()(x5)
+    x6 = Activation('relu')(x6)
+    x6 = Conv2D(128, (3, 3), strides=[2,2], padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x6)
+
+    x6 = keras.layers.normalization.BatchNormalization()(x6)
+    x6 = Activation('relu')(x6)
+    x6 = Conv2D(128, (3, 3), padding='same',
+                kernel_initializer="he_normal",
+                kernel_regularizer=regularizers.l2(1e-4))(x6)
+
+    x5 = Conv2D(128, (1, 1), strides=[2,2], padding='same')(x5)
+    x7 = add([x5, x6])
+
+    x = Flatten()(x7)
+    x = Activation('relu')(x)
+    x = Dropout(0.5)(x)
+   
+    # RNN block
+    
+    # Steering channel
+    steer = Dense(output_dim)(x)
+
+    # Collision channel
+    coll = Dense(output_dim)(x)
+    coll = Activation('sigmoid')(coll)
+
+    # Define steering-collision model
+    model = Model(inputs=[img_input], outputs=[steer, coll])
+    print(model.summary())
+
+    return model
\ No newline at end of file
diff --git a/data_preprocessing/time_stamp_matching.py b/data_preprocessing/time_stamp_matching.py
index 19b48ef..9e6446a 100644
--- a/data_preprocessing/time_stamp_matching.py
+++ b/data_preprocessing/time_stamp_matching.py
@@ -10,7 +10,8 @@ import re
 import os
 
 # Path to the data extracted from the Udacity dataset
-folder = None #"training"  or "testing"
+# folder = None #"training"  or "testing"
+folder = "datasets/steering_dataset/testing"
 assert folder, "You should provide the dataset folder"
 experiments = glob.glob(folder + "/*")
 
@@ -50,10 +51,10 @@ def getSyncSteering(fname, idx):
 # For every bag...
 for exp in experiments:
     # Read images
-    images = [os.path.basename(x) for x in glob.glob(exp + "/images/*.png")]
+    images = [os.path.basename(x) for x in glob.glob(exp + "/images/*.jpg")]
     im_stamps = []
     for im in images:
-        stamp = int(re.sub(r'\.png$', '', im))
+        stamp = int(re.sub(r'\.jpg$', '', im))
         im_stamps.append(stamp)
     im_stamps = np.array(sorted(im_stamps))
 
@@ -64,7 +65,8 @@ for exp in experiments:
     # Time-stamp matching between images and steerings
     match_stamp, match_idx = getMatching(im_stamps, steer_stamps)
     match_idx = np.array(match_idx)
-    match_idx = match_idx[:,0]
+    print(match_idx[:10],match_idx.shape)
+    # match_idx = match_idx[:,0]
 
     # Get matched commands
     original_fname = exp + "/interpolated.csv"
diff --git a/drone_control/dronet/README.md b/drone_control/dronet/README.md
index 9466ed9..98a37cd 100644
--- a/drone_control/dronet/README.md
+++ b/drone_control/dronet/README.md
@@ -15,7 +15,7 @@ It is necessary for you to install [ROS](http://wiki.ros.org/ROS/Installation) t
 ### Step 2: Build your workspace
 
 The folder containing all the related code for a project is usually defined as `workspace'.
-Create your own workspace following [these instructions](http://wiki.ros.org/catkin/Tutorials/create_a_workspace) and call it ```bebop_ws'''.
+Create your own workspace following [these instructions](http://wiki.ros.org/catkin/Tutorials/create_a_workspace) and call it ```bebop_ws```.
 
 ### Step 3: Setup Bebop Autonomy
 
@@ -166,4 +166,4 @@ There might be some variables you would like to tune to get the maximal performa
 The most important parameters are _SpeedSettingsMaxRotationSpeedCurrent_, _PilotingSettingsMaxTiltCurrent_, _SpeedSettingsOutdoorOutdoor_.
 
 Additionally, you might want to tune the control pipeline accordingly. To do this, modify the file [deep_navigation.launch](./dronet_control/launch/deep_navigation.launch).
-Here, the most important parameter to tune is _critical\_prob_. For low values, the drone will be very conservative. For high values, it will stop only very close to obstacles.
+Here, the most important parameter to tune is _critical\_prob_. For low values, the drone will be very conservative. For high values, it will stop only very close to obstacles.
\ No newline at end of file
